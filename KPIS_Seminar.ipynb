{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOj5JVRMeqUq"
      },
      "source": [
        "### Wandb install and login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZDz2bj8VPGS"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb\n",
        "# !wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na-LsWXhh7we"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aTHbma8-nLd"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3\n",
        "!pip install keras-rl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqtRx1JeBLxU"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import google.colab.drive as gdrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "gdrive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WXOBlFRnujR"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import bisect\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.dqn import DQN\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes, EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "import os.path\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents import SARSAAgent\n",
        "from rl.policy import EpsGreedyQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbaIyBBqW8DG"
      },
      "outputs": [],
      "source": [
        "# import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uboeqisph_yD"
      },
      "source": [
        "### CartPole Environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Oho3m5mCGkp"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPgs1Hkt7JYS"
      },
      "source": [
        "### Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQCx9DT2nYZY"
      },
      "outputs": [],
      "source": [
        "def random_search(episodes_num):\n",
        "  \"\"\"Runs cartpole test with random actions.\n",
        "  \"\"\"\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  episode_scores = []\n",
        "  episode_counter = 0\n",
        "  \n",
        "  for episode in range(episodes_num):\n",
        "    episode_counter += 1\n",
        "    score = 0\n",
        "    done = False\n",
        "    current_observation = env.reset()\n",
        "\n",
        "    while not done and score < 200:\n",
        "      # select random sample from action space\n",
        "      action = env.action_space.sample()\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      current_observation = next_observation\n",
        "\n",
        "    # episode finished -> cleanup\n",
        "    # save score\n",
        "    episode_scores.append(score)\n",
        "\n",
        "    # mark first episode that keeps the pole balanced for the entire episode\n",
        "    if score >= 200:\n",
        "      break\n",
        "      \n",
        "\n",
        "  # test run cleanup, close environment and return metrics\n",
        "  env.close()\n",
        "  return episode_scores, episode_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OGuBVgP29r"
      },
      "source": [
        "### Random Search with weights\n",
        "As seen here https://github.com/kvfrans/openai-cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3EC37sw7LuM"
      },
      "outputs": [],
      "source": [
        "def random_search_weights(episodes_num):\n",
        "  \"\"\"Runs cartpole environment with random weights per episode.\n",
        "  \"\"\"\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  episode_scores = []\n",
        "  episode_counter = 0\n",
        "  \n",
        "  for episode in range(episodes_num):\n",
        "    episode_counter += 1\n",
        "    score = 0\n",
        "    done = False\n",
        "    current_observation = env.reset()\n",
        "    action_policy = np.random.rand(4) * 2 -1\n",
        "    # done = True , score =10\n",
        "    while not done and score < 500:\n",
        "      # select random sample from action space\n",
        "      action = 0 if np.matmul(action_policy, current_observation) < 0 else 1\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      current_observation = next_observation\n",
        "      #print(score)\n",
        "    # episode finished -> cleanup\n",
        "    # save score\n",
        "    episode_scores.append(score)\n",
        "\n",
        "    # mark first episode that keeps the pole balanced for the entire episode\n",
        "    if score >= 200:\n",
        "      break\n",
        "\n",
        "  # test run cleanup, close environment and return metrics\n",
        "  env.close()\n",
        "  return episode_scores, episode_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoU9UJ5y78ya"
      },
      "outputs": [],
      "source": [
        "def CartPole_Random_Search(runs = 200, episodes = 1000):\n",
        "  rewards = []\n",
        "  counters = []\n",
        "  for i in range(runs):\n",
        "    episode_rewards, first_successful_episode = random_search(episodes_num=episodes)\n",
        "    print(\"Run: \", i, first_successful_episode)\n",
        "    rewards.append(episode_rewards)\n",
        "    counters.append(first_successful_episode)\n",
        "\n",
        "  return rewards, counters\n",
        "\n",
        "def CartPole_Random_Weights_Search(runs = 200, episodes = 1000):\n",
        "  rewards = []\n",
        "  counters = []\n",
        "  for i in range(runs):\n",
        "    episode_rewards, first_successful_episode = random_search_weights(episodes_num=episodes)\n",
        "    print(\"Run: \", i, \" Needed episodes: \", first_successful_episode)\n",
        "    rewards.append(episode_rewards)\n",
        "    counters.append(first_successful_episode)\n",
        "\n",
        "  return rewards, counters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaHodpb44V9r"
      },
      "outputs": [],
      "source": [
        "def plot_durations(durations, episodes, title, label):\n",
        "  \"\"\"Creates histogramm of durations data.\n",
        "  \"\"\"\n",
        "  bins = np.arange(0, 100, 10)\n",
        "  plt.title(title)\n",
        "  plt.hist(durations, bins=bins, label=label, alpha=0.75)\n",
        "  plt.xlabel(\"Required episodes to first reach a reward of 200\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  #plt.savefig(\"/content/drive/MyDrive/\" + label + \"-speed.png\")\n",
        "\n",
        "def plot_rewards(rewards, title, label):\n",
        "  plt.title(title)\n",
        "  plt.plot(rewards, alpha=0.75)\n",
        "  plt.xlabel(\"Epsiodes\")\n",
        "  plt.ylabel(\"Average Reward\")\n",
        "  plt.show()\n",
        "\n",
        "def eval_random_search(runs = 1000, episodes = 10000):\n",
        "  \"\"\"Evaluates the random algorithms\n",
        "  \"\"\"  \n",
        "  #rewards, durations = CartPole_Random_Search(runs, episodes)\n",
        "  rewards_weights, durations_weights = CartPole_Random_Weights_Search(runs, episodes)\n",
        "  np.save('/content/drive/MyDrive/kpis-graphs/random-1.npy', durations_weights)\n",
        "  #plot_durations(durations, episodes, \"Random Search Speed\", \"random\")\n",
        "  #print(np.sum(durations_weights)/10000.0)\n",
        "  #plot_durations(durations_weights, episodes, \"Random Policy Search Speed\", \"policy\")\n",
        "  \n",
        "  # aggregated_rewards = np.average(np.array(rewards).reshape(runs, -1, 50), axis=2)\n",
        "  # average_rewards = np.average(aggregated_rewards, axis=0)\n",
        "  # plot_rewards(average_rewards, \"Average Reward per episode for Random Search\", \"random\")\n",
        "\n",
        "  # aggregated_rewards_weights = np.average(np.array(rewards_weights).reshape(runs, -1, 50), axis=2)\n",
        "  # average_rewards_weights = np.average(aggregated_rewards_weights, axis=0)\n",
        "  # plot_rewards(average_rewards_weights, \"Average Reward per episode for Random Policy Search\", \"policy\")\n",
        "eval_random_search(runs=100, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN8PZ4Eb8Eip"
      },
      "outputs": [],
      "source": [
        "r, d = CartPole_Random_Search(100, 1000)\n",
        "np.save('/content/drive/MyDrive/kpis-graphs/control-1.npy', d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t6dBjt0DcwD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9xv3OWQ_Rp4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agxGDF0gUQAS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqIPXsLnhH8a"
      },
      "source": [
        "### Q-Learning and SARSA Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkwHBffdwBn0"
      },
      "outputs": [],
      "source": [
        "def initialization(observation_num = 5,action_num = 2):\n",
        "  value_function = np.random.uniform(low=--1,high=1,size=(observation_num,observation_num,observation_num,observation_num,action_num))\n",
        "  discrete_observation_space =  {\"Cart Position\": np.linspace(-2.4,2.4,observation_num),\n",
        "                                 \"Cart Velocity\": np.linspace(-5,5,observation_num),\n",
        "                                 \"Pole Angle\": np.linspace(-0.2095,0.2095,observation_num),\n",
        "                                 \"Pole Angular Velocity\": np.linspace(-5,5,observation_num)}\n",
        "  return value_function,discrete_observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMAgnu_d5sLL"
      },
      "outputs": [],
      "source": [
        "def continuous2discrete(continuous_observation,discrete_observation_space):\n",
        "  discrete_observation = []\n",
        "  for i,key in enumerate(discrete_observation_space.keys()):\n",
        "    discrete_observation.append(bisect.bisect(discrete_observation_space[key],continuous_observation[i])-1)\n",
        "  return discrete_observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6O2ccztudgC"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(env,epsilon,value_function,current_observation):\n",
        "  explore_or_exploit = np.random.choice(a=[\"explore\",\"exploit\"],p=[epsilon,1-epsilon])\n",
        "  if explore_or_exploit == \"explore\":\n",
        "    return env.action_space.sample()\n",
        "  else:\n",
        "    return np.argmax(value_function[current_observation])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_PmZtn8nDRl"
      },
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfLTlhGes-jz"
      },
      "outputs": [],
      "source": [
        "def q_learning(gamma,alpha,epsilon,episodes_num):\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  value_function,discrete_observation_space = initialization()\n",
        "  max_score = 0\n",
        "  episode_scores = []\n",
        "  episode_counter = 0\n",
        "  for episode in range(episodes_num):\n",
        "    episode_counter += 1\n",
        "    score = 0\n",
        "    done = False\n",
        "    current_observation_continuous = env.reset()\n",
        "    current_observation = tuple(continuous2discrete(current_observation_continuous,discrete_observation_space))\n",
        "\n",
        "    while not done and score < 200:\n",
        "      action = epsilon_greedy(env,epsilon,value_function,current_observation)\n",
        "      next_observation_continuous, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      next_observation = tuple(continuous2discrete(next_observation_continuous,discrete_observation_space))\n",
        "      value_function[current_observation][action] += alpha*(reward + gamma*np.max(value_function[next_observation]) - value_function[current_observation][action])\n",
        "      current_observation = next_observation\n",
        "    \n",
        "    episode_scores.append(score)\n",
        "\n",
        "    if score >= 200:\n",
        "      break\n",
        "    \n",
        "  env.close()\n",
        "  return episode_scores, episode_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GViiCZHeJlO8"
      },
      "outputs": [],
      "source": [
        "def CartPole_Q_Learning(runs= 200, episodes = 10000):\n",
        "  rewards = []\n",
        "  counters = []\n",
        "  for i in range(runs):\n",
        "    episode_rewards, first_successful_episode = q_learning(gamma=0.95,alpha=0.1,epsilon=0.1,episodes_num=episodes)\n",
        "    print(\"Run: \", i, \"Counter: \", first_successful_episode)\n",
        "    rewards.append(episode_rewards)\n",
        "    counters.append(first_successful_episode)\n",
        "  return rewards, counters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xal9R6DJ_KD"
      },
      "outputs": [],
      "source": [
        "# s, results = CartPole_Q_Learning(runs=4)\n",
        "# plt.hist(results, 50, facecolor='g', alpha=0.75)\n",
        "# plt.xlabel(\"Required episodes to reach 200\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# #plt.savefig('/content/drive/MyDrive/KPIS-Seminar-DQN-Model/test.png')\n",
        "# print(np.sum(results)/1000.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cry5pPmSta4c"
      },
      "outputs": [],
      "source": [
        "def eval_q_learning(runs = 1000, episodes = 10000):\n",
        "  \"\"\"Evaluates the random algorithms\n",
        "  \"\"\"    \n",
        "  rewards, counters = CartPole_Q_Learning(runs, episodes)\n",
        "  np.save('/content/drive/MyDrive/kpis-graphs/q-learning-1.npy', counters)\n",
        "  #print(np.sum(counters)/1000.0)\n",
        "\n",
        "  #plot_durations(counters, episodes, \"Q-Learning Speed\", \"Q\")\n",
        "  \n",
        "  #aggregated_rewards = np.average(np.array(rewards).reshape(runs, -1, 50), axis=2)\n",
        "  #average_rewards = np.average(np.array(rewards), axis=0)\n",
        "  #plot_rewards(average_rewards, \"Average Reward per episode for Q-Learning\", \"Q\")\n",
        "eval_q_learning(runs=100, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-TwvP5duxOY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIUVYnGWu1hC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oj4lXANnJVZ"
      },
      "source": [
        "### SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8IfFNhwnAXk"
      },
      "outputs": [],
      "source": [
        "def sarsa(gamma,alpha,epsilon,episodes_num):\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  value_function,discrete_observation_space = initialization()\n",
        "  max_score = 0\n",
        "  scores = []\n",
        "  counter = 0 \n",
        "  for episode in range(episodes_num):\n",
        "    counter += 1\n",
        "    score = 0\n",
        "    done = False\n",
        "    current_observation_continuous = env.reset()\n",
        "    current_observation = tuple(continuous2discrete(current_observation_continuous,discrete_observation_space))\n",
        "    current_step = 0\n",
        "    action = epsilon_greedy(env, epsilon,value_function,current_observation)\n",
        "\n",
        "    while not done and current_step < 200:\n",
        "      #env.render()\n",
        "      next_observation_continuous, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      next_observation = tuple(continuous2discrete(next_observation_continuous,discrete_observation_space))\n",
        "      next_action = epsilon_greedy(env, epsilon,value_function,next_observation)\n",
        "      value_function[current_observation][action] += alpha*(reward + gamma*value_function[next_observation][next_action] - value_function[current_observation][action])\n",
        "      current_observation = next_observation\n",
        "      action = next_action\n",
        "      current_step += 1\n",
        "      \n",
        "    scores.append(score)\n",
        "    \n",
        "    if max_score < score:\n",
        "      max_score = score\n",
        "    if score >= 200:\n",
        "      break \n",
        "    \n",
        "  env.close()\n",
        "  return scores, counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdrRmpO8zvgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El07RARhz9a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH1EZ2zrqKuy"
      },
      "outputs": [],
      "source": [
        "def CartPole_Sarsa(runs = 1000, episodes = 10000):\n",
        "  #wandb.init(project=\"Q-Learning\", entity=\"ml-experiments\")\n",
        "  results = []\n",
        "  for i in range(runs):\n",
        "    scores, counter = sarsa(gamma=0.95,alpha=0.1,epsilon=0.1,episodes_num=episodes)\n",
        "    print(\"Run: \", i, \"Counter: \", counter)\n",
        "    results.append(counter)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOoYhHBCDodo"
      },
      "outputs": [],
      "source": [
        "def eval_sarsa(runs = 1000, episodes = 10000):\n",
        "  \"\"\"Evaluates the random algorithms\n",
        "  \"\"\"    \n",
        "  counters = CartPole_Sarsa(runs, episodes)\n",
        "  #print(np.sum(counters)/1000.0)\n",
        "  \n",
        "  np.save('/content/drive/MyDrive/kpis-graphs/sarsa-1.npy', counters)\n",
        "\n",
        "  #plot_durations(counters, episodes, \"SARSA Speed\", \"sarsa\")\n",
        "  \n",
        "  #aggregated_rewards = np.average(np.array(rewards).reshape(runs, -1, 50), axis=2)\n",
        "  #average_rewards = np.average(np.array(rewards), axis=0)\n",
        "  #plot_rewards(average_rewards, \"Average Reward per episode for Q-Learning\", \"Q\")\n",
        "eval_sarsa(runs=100, episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyenkth1qOYJ"
      },
      "outputs": [],
      "source": [
        "# results = CartPole_Sarsa()\n",
        "# plt.hist(results, 50, facecolor='g', alpha=0.75)\n",
        "# plt.xlabel(\"Required episodes to reach 200\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.show()\n",
        "# print(np.sum(results)/1000.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcIpEg2p_YwI"
      },
      "source": [
        "### DQN (stable_baselines3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzlzqYJj_Wm9"
      },
      "outputs": [],
      "source": [
        "def train_model(env, timesteps,lr,gamma):\n",
        "  callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=1000, verbose=1)\n",
        "  model = DQN(policy=\"MlpPolicy\", env=env, learning_rate=lr, gamma=gamma)\n",
        "  model.learn(total_timesteps=timesteps, callback=callback_max_episodes)  \n",
        "  #model.save('/content/drive/MyDrive/KPIS-Seminar-DQN-Model/stable_baselines3_dqn_cartpole')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BCpujvjB3Di"
      },
      "outputs": [],
      "source": [
        "def stable_baselines3_dqn(timesteps,gamma,lr,episodes_num):\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  if os.path.isfile('/content/drive/MyDrive/KPIS-Seminar-DQN-Model/stable_baselines3_dqn_cartpole.zip'):\n",
        "    print(\"Loading the model\")\n",
        "    model = DQN.load('/content/drive/MyDrive/KPIS-Seminar-DQN-Model/stable_baselines3_dqn_cartpole')\n",
        "  else:\n",
        "    print(\"Training model\")\n",
        "    model = train_model(env,timesteps,lr,gamma)\n",
        "  \n",
        "  print(\"Loading finished\")\n",
        "  max_score = 0\n",
        "  scores = []\n",
        "  counter = 0 \n",
        "  for episode in range(episodes_num):\n",
        "    counter += 1\n",
        "    score = 0\n",
        "    done = False\n",
        "    current_step = 0\n",
        "    current_observation = env.reset()\n",
        "\n",
        "    while not done and current_step < 200:\n",
        "      #env.render()\n",
        "      action, used_state = model.predict(current_observation, deterministic=False)\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      current_observation = next_observation\n",
        "      current_step += 1\n",
        "    \n",
        "    scores.append(score)\n",
        "    \n",
        "    if max_score < score:\n",
        "      max_score = score\n",
        "      if score >= 200: \n",
        "        break \n",
        "    \n",
        "  env.close()\n",
        "  return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jElRpfrESA-"
      },
      "outputs": [],
      "source": [
        "def CartPole_DQN():\n",
        "  #wandb.init(project=\"Q-Learning\", entity=\"ml-experiments\")\n",
        "  results = []\n",
        "  for i in range(1000):\n",
        "    counter = stable_baselines3_dqn(timesteps=300000,gamma=0.95,lr=0.0001,episodes_num=10000)\n",
        "    print(\"Run: \", i, \"Counter: \", counter)\n",
        "    results.append(counter)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD195DnpbuQD"
      },
      "outputs": [],
      "source": [
        "# results = CartPole_DQN()\n",
        "# plt.hist(results, 50, facecolor='g', alpha=0.75)\n",
        "# plt.xlabel(\"Required episodes to reach 200\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.show()\n",
        "# print(np.sum(results)/1000.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTEx64RElqL"
      },
      "outputs": [],
      "source": [
        "#stable_baselines3_dqn(timesteps=300000,gamma=0.95,lr=0.0001,episodes_num=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXL_ixfgewx3"
      },
      "outputs": [],
      "source": [
        "#plot_durations(results, 1000, \"DQN Speed\", \"dqn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_V83FrwTOh0"
      },
      "outputs": [],
      "source": [
        "log_dir = \"./drive/MyDrive/kpis-logs/dqn\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env = Monitor(env, log_dir)\n",
        "\n",
        "\n",
        "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=10000, verbose=1)\n",
        "model = DQN(policy=\"MlpPolicy\", env=env, learning_rate=0.1, gamma=0.95)\n",
        "model.learn(total_timesteps=200000, log_interval=1, callback=[callback_max_episodes])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rqcDt4sTlkn"
      },
      "outputs": [],
      "source": [
        "#load_results(log_dir)\n",
        "np.amax(env.get_episode_lengths())\n",
        "#x, y = ts2xy(, \"timesteps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft7MVIVWcGAs"
      },
      "outputs": [],
      "source": [
        "log_dir = \"./drive/MyDrive/kpis-logs/dqn\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "counter = []\n",
        "for i in range(50):\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  env.reset()\n",
        "  env = Monitor(env, log_dir)\n",
        "  callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=10000, verbose=1)\n",
        "  model = DQN(policy=\"MlpPolicy\", env=env, learning_rate=0.01, gamma=0.95)\n",
        "  model.learn(total_timesteps=200000, log_interval=1, callback=[callback_max_episodes])\n",
        "  first_episode = 10000\n",
        "  amax = np.amax(env.get_episode_lengths())\n",
        "  if amax >= 200:\n",
        "    first_episode = np.argmax(env.get_episode_lengths())\n",
        "  counter.append(first_episode)\n",
        "  print(\"i: \", i, \" counter: \", first_episode, \" amax: \", amax)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/kpis-graphs/dqn-1.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJlRVc09xBgA"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/drive/MyDrive/kpis-graphs/dqn-1.npy\", counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-4gSo9dqj6w"
      },
      "source": [
        "### Deep Sarsa from this example: [keras-rl/examples/sarsa_cartpole.py](https://github.com/keras-rl/keras-rl/blob/master/examples/sarsa_cartpole.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF-wVsAb2Bv0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1utlnnsMqkcK"
      },
      "outputs": [],
      "source": [
        "# Example from https://github.com/keras-rl/keras-rl/blob/master/examples/sarsa_cartpole.py\n",
        "\n",
        "def buil_model(env):\n",
        "  \n",
        "  nb_actions = env.action_space.n\n",
        "\n",
        "  # Next, we build a very simple model.\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(nb_actions))\n",
        "  model.add(Activation('linear'))\n",
        "  print(model.summary())\n",
        "\n",
        "  policy = EpsGreedyQPolicy()\n",
        "  sarsa = SARSAAgent(model=model, policy=policy, gamma=0.95, nb_actions=nb_actions, nb_steps_warmup=10)\n",
        "  sarsa.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "  return sarsa\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(42)\n",
        "sarsa = buil_model(env)\n",
        "\n",
        "training_history = sarsa.fit(env, nb_steps=10000, nb_max_episode_steps=200, visualize=False, verbose=1)\n",
        "sarsa.save_weights('/content/drive/MyDrive/KPIS-Seminar-Sarsa-Model/sarsa.hdf5', overwrite=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWNXMXkPgupS"
      },
      "outputs": [],
      "source": [
        "sarsa.save_weights('/content/drive/MyDrive/KPIS-Seminar-Sarsa-Model/sarsa.hdf5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBs0kiPb2Mo"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(42)\n",
        "model = buil_model(env)\n",
        "\n",
        "if os.path.isfile('/content/drive/MyDrive/KPIS-Seminar-Sarsa-Model/sarsa.hdf5'):\n",
        "  print('found model')\n",
        "  model.load_weights('/content/drive/MyDrive/KPIS-Seminar-Sarsa-Model/sarsa.hdf5')\n",
        "\n",
        "test_data = model.test(env, nb_episodes=1000, nb_max_episode_steps=200, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjbUEHHHhyd7"
      },
      "outputs": [],
      "source": [
        "counter = test_data.history['nb_steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dwkm7FsjODw"
      },
      "outputs": [],
      "source": [
        "plot_durations(counter, 1000, \"DSN Speed\", \"dsn\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKHnMfQ21e0J"
      },
      "outputs": [],
      "source": [
        "print(np.sum(counter)/1000.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueL-Oi2x1t6u"
      },
      "outputs": [],
      "source": [
        "def eval_dsn(runs = 1000, episodes = 10000):\n",
        "  \"\"\"Evaluates the random algorithms\n",
        "  \"\"\"    \n",
        "  #counters = CartPole_Sarsa(runs, episodes)\n",
        "  results = []\n",
        "  for i in range(runs):\n",
        "    #counter = stable_baselines3_dqn(timesteps=300000,gamma=0.95,lr=0.0001,episodes_num=episodes)\n",
        "    res = model.test(env, nb_episodes=episodes, nb_max_episode_steps=200, visualize=False)\n",
        "    counter = np.argmax(res.history['nb_steps'])\n",
        "    print(\"Run: \", i, \"Counter: \", counter)\n",
        "    results.append(counter)\n",
        "  return results\n",
        "  print(np.sum(counters)/1000.0)\n",
        "\n",
        "  plot_durations(counters, episodes, \"DSN Speed\", \"dsn\")\n",
        "  \n",
        "  \n",
        "eval_dsn(runs=1000, episodes=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vT5bpV39apw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15 (default, Oct 12 2022, 19:14:39) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
